{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo se leeran los datos obtenidos de las caracteristicas propuestas\n",
    "\n",
    "Se utilizará la librería de PANDAS para realizar este trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: True\n",
      "Nombre de la GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU disponible:\", torch.cuda.is_available())\n",
    "print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 32]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 32]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 32]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 32]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 32]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 32]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 32]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 32]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [128, 64, 32]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [128, 64, 32]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [128, 64, 32]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [128, 64, 32]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [128, 64, 32]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [128, 64, 32]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [128, 64, 32]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [128, 64, 32]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 64]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 64]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 64]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 64]\n",
      "Tasa de aprendizaje: 0.001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 64]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 64]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 16\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 64]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.3\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Entrenando con:\n",
      "Capas ocultas: [64, 64]\n",
      "Tasa de aprendizaje: 0.0001\n",
      "Tamaño de lote: 32\n",
      "Tasa de dropout: 0.5\n",
      "Total de videos cargados: 49\n",
      "Videos normales: 22\n",
      "Videos sospechosos: 27\n",
      "Longitudes de videos - Min: 173, Max: 10000\n",
      "\n",
      "Mejores resultados:\n",
      "   hidden_layers  learning_rate  batch_size  dropout_rate  val_accuracy\n",
      "0       [64, 32]         0.0010          16           0.3          60.0\n",
      "1       [64, 32]         0.0010          16           0.5          60.0\n",
      "22      [64, 64]         0.0001          32           0.3          60.0\n",
      "21      [64, 64]         0.0001          16           0.5          60.0\n",
      "20      [64, 64]         0.0001          16           0.3          60.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, csv_dir, selected_features=None, max_frames=None):\n",
    "        \"\"\"\n",
    "        Conjunto de datos para clasificación de videos con longitudes variables\n",
    "        \n",
    "        Args:\n",
    "            csv_dir (str): Directorio con archivos CSV de videos\n",
    "            selected_features (list, optional): Lista de características a usar\n",
    "            max_frames (int, optional): Número máximo de frames a considerar\n",
    "        \"\"\"\n",
    "        self.videos = []\n",
    "        self.labels = []\n",
    "        self.video_lengths = []\n",
    "        \n",
    "        # Mapeo de etiquetas más flexible\n",
    "        for filename in os.listdir(csv_dir):\n",
    "            if filename.endswith('.csv'):\n",
    "                filepath = os.path.join(csv_dir, filename)\n",
    "                \n",
    "                # Leer CSV con encabezados\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # Selección de características\n",
    "                if selected_features is None:\n",
    "                    # Si no se especifican, usar todas las características numéricas\n",
    "                    features = df.select_dtypes(include=[np.number])\n",
    "                else:\n",
    "                    # Usar solo las características especificadas\n",
    "                    features = df[selected_features]\n",
    "                \n",
    "                # Convertir a numpy array\n",
    "                features_array = features.values\n",
    "                \n",
    "                # Limitar número de frames si se especifica\n",
    "                if max_frames is not None:\n",
    "                    features_array = features_array[:max_frames]\n",
    "                \n",
    "                # Mapeo de etiquetas basado en prefijo del nombre del archivo\n",
    "                if filename.startswith('normal_'):\n",
    "                    label = 0\n",
    "                elif filename.startswith('sospechoso_'):\n",
    "                    label = 1\n",
    "                else:\n",
    "                    print(f\"Advertencia: Archivo {filename} ignorado - etiqueta no reconocida\")\n",
    "                    continue\n",
    "                \n",
    "                # Normalización de características del video\n",
    "                scaler = StandardScaler()\n",
    "                normalized_features = scaler.fit_transform(features_array)\n",
    "                \n",
    "                # Convertir a tensor de PyTorch\n",
    "                video_tensor = torch.FloatTensor(normalized_features)\n",
    "                \n",
    "                self.videos.append(video_tensor)\n",
    "                self.labels.append(label)\n",
    "                self.video_lengths.append(len(video_tensor))\n",
    "        \n",
    "        # Verificar que se hayan cargado videos\n",
    "        if not self.videos:\n",
    "            raise ValueError(\"No se encontraron videos válidos. Verifica tus archivos CSV.\")\n",
    "        \n",
    "        print(f\"Total de videos cargados: {len(self.videos)}\")\n",
    "        print(f\"Videos normales: {self.labels.count(0)}\")\n",
    "        print(f\"Videos sospechosos: {self.labels.count(1)}\")\n",
    "        print(f\"Longitudes de videos - Min: {min(self.video_lengths)}, Max: {max(self.video_lengths)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.videos[idx], torch.LongTensor([self.labels[idx]])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Función personalizada para manejar lotes con longitudes variables\n",
    "    \"\"\"\n",
    "    videos, labels = zip(*batch)\n",
    "    padded_videos = pad_sequence(videos, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return padded_videos, labels\n",
    "\n",
    "class VideoClassificationMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, dropout_rate=0.3):\n",
    "        super(VideoClassificationMLP, self).__init__()\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        \n",
    "        return self.model(x)\n",
    "\n",
    "def train_video_classifier(csv_dir, \n",
    "                           selected_features=None,\n",
    "                           max_frames=None,\n",
    "                           hidden_layers=[64, 32], \n",
    "                           learning_rate=0.001, \n",
    "                           epochs=100, \n",
    "                           batch_size=16,\n",
    "                           dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Entrenar clasificador de videos\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Cargar datos\n",
    "    dataset = VideoFrameDataset(csv_dir, selected_features, max_frames)\n",
    "    \n",
    "    # Dividir datos\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(dataset)), test_size=0.2, stratify=dataset.labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Crear subconjuntos\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "    # Crear DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    # Configurar modelo\n",
    "    input_size = dataset.videos[0].shape[1]\n",
    "    num_classes = 2\n",
    "    \n",
    "    model = VideoClassificationMLP(input_size, hidden_layers, num_classes, dropout_rate).to(device)\n",
    "    \n",
    "    # Configurar entrenamiento\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch_videos, batch_labels in train_loader:\n",
    "            batch_videos = batch_videos.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_videos)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_predictions += batch_labels.size(0)\n",
    "            correct_predictions += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_videos, batch_labels in val_loader:\n",
    "                batch_videos = batch_videos.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                \n",
    "                outputs = model(batch_videos)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        # Calcular métricas\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct_predictions / total_predictions\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        # Actualizar mejor modelo\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    return best_val_accuracy\n",
    "\n",
    "def grid_search(csv_dir, \n",
    "                hidden_layers_options=[[64, 32], [128, 64, 32], [64, 64]],\n",
    "                learning_rates=[0.001, 0.0001],\n",
    "                batch_sizes=[16, 32],\n",
    "                dropout_rates=[0.3, 0.5]):\n",
    "    \"\"\"\n",
    "    Realizar grid search sobre hiperparámetros\n",
    "    \"\"\"\n",
    "    # Generar todas las combinaciones de hiperparámetros\n",
    "    hyperparameter_combinations = list(itertools.product(\n",
    "        hidden_layers_options, \n",
    "        learning_rates, \n",
    "        batch_sizes,\n",
    "        dropout_rates\n",
    "    ))\n",
    "    \n",
    "    # Almacenar resultados del grid search\n",
    "    results = []\n",
    "    \n",
    "    # Iterar sobre combinaciones de hiperparámetros\n",
    "    for hidden_layers, lr, batch_size, dropout_rate in hyperparameter_combinations:\n",
    "        print(\"\\nEntrenando con:\")\n",
    "        print(f\"Capas ocultas: {hidden_layers}\")\n",
    "        print(f\"Tasa de aprendizaje: {lr}\")\n",
    "        print(f\"Tamaño de lote: {batch_size}\")\n",
    "        print(f\"Tasa de dropout: {dropout_rate}\")\n",
    "        \n",
    "        val_accuracy = train_video_classifier(\n",
    "            csv_dir=csv_dir,\n",
    "            hidden_layers=hidden_layers,\n",
    "            learning_rate=lr,\n",
    "            batch_size=batch_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            epochs=200,  # Reducir épocas para grid search\n",
    "            max_frames=10000\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'val_accuracy': val_accuracy\n",
    "        })\n",
    "    \n",
    "    # Ordenar resultados por precisión de validación\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('val_accuracy', ascending=False)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results_df.to_csv('grid_search_results.csv', index=False)\n",
    "    print(\"\\nMejores resultados:\")\n",
    "    print(results_df.head())\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Directorio con archivos CSV de videos\n",
    "    csv_directory = '../datasetCSV/'\n",
    "    \n",
    "    # Realizar grid search\n",
    "    grid_search_results = grid_search(csv_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornoTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
